import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
import re
import random
from collections import Counter
from konlpy.tag import Okt
import json
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# ëª¨ë¸ ë¡œë”©
finbert_tokenizer = AutoTokenizer.from_pretrained("snunlp/KR-FinBert-SC")
finbert_model = AutoModelForSequenceClassification.from_pretrained("snunlp/KR-FinBert-SC")
finbert_pipe = pipeline("text-classification", model=finbert_model, tokenizer=finbert_tokenizer, return_all_scores=True)

electra_tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")
electra_model = AutoModelForSequenceClassification.from_pretrained("beomi/KcELECTRA-base", num_labels=3)

# í¬ë¡¤ë§ ê´€ë ¨
headers = {"User-Agent": "Mozilla/5.0"}


def get_url(item_code, page_no=1):
    return f"https://finance.naver.com/item/board.nhn?code={item_code}&page={page_no}"


def get_one_page(item_code, page_no):
    url = get_url(item_code, page_no)
    response = requests.get(url, headers=headers)
    html = BeautifulSoup(response.text, "lxml")

    tables = pd.read_html(response.text)
    board_table = next((t for t in tables if 'ë‚ ì§œ' in t.columns and 'ì œëª©' in t.columns), None)
    if board_table is None:
        raise ValueError("ê²Œì‹œíŒ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    table_filtered = board_table[['ë‚ ì§œ', 'ì œëª©']].copy()
    today = datetime.today().strftime('%Y.%m.%d')
    table_filtered['ë‚ ì§œ'] = table_filtered['ë‚ ì§œ'].astype(str).str.split().str[0]
    return table_filtered[table_filtered['ë‚ ì§œ'] == today]


def get_last_page(item_code):
    url = get_url(item_code)
    response = requests.get(url, headers=headers)
    html = BeautifulSoup(response.text, "lxml")
    try:
        last_page = int(
            html.select_one(
                "#content > div.section.inner_sub > table > tbody > tr > td > table > tbody > tr > td.pgRR > a"
            )["href"].split('=')[-1]
        )
    except (TypeError, AttributeError):
        last_page = 1
    return last_page


def get_all_pages(item_code):
    last = get_last_page(item_code)
    page_list = []

    for page_num in range(1, min(last, 10) + 1):
        try:
            page = get_one_page(item_code, page_num)
            if not page.empty:
                page_list.append(page)
        except Exception as e:
            print(f"{item_code}ì˜ {page_num}í˜ì´ì§€ ì—ëŸ¬: {e}")
            continue

    if page_list:
        df_all_page = pd.concat(page_list, ignore_index=True)
        return df_all_page.dropna(how="all")
    else:
        return pd.DataFrame(columns=['ë‚ ì§œ', 'ì œëª©'])


# ì „ì²˜ë¦¬
okt = Okt()


def clean_text(text):
    text = BeautifulSoup(text, "html.parser").get_text()
    text = re.sub(r"[^ã„±-ã…ã…-ã…£ê°€-í£\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def preprocess_text(text):
    text = clean_text(text)
    tokens = [word for word in okt.morphs(text) if word not in ['ì˜', 'ì—', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ë¥¼', 'ì„', 'ë„', 'ë¡œ']]
    return " ".join(tokens)


def convert_label(label):
    return {"positive": 1, "neutral": 0, "negative": -1}.get(label, None)


# ë‘ ëª¨ë¸ì„ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê°ì„± ë¶„ì„;
def analyze_sentiments_with_dual_model(texts, threshold=0.6):
    finbert_preds = finbert_pipe(texts, truncation=True)

    electra_inputs = electra_tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        electra_outputs = electra_model(**electra_inputs)
    electra_logits = electra_outputs.logits
    electra_probs = torch.nn.functional.softmax(electra_logits, dim=-1)
    electra_labels = torch.argmax(electra_probs, dim=-1).tolist()

    final_results = []
    for i, pred in enumerate(finbert_preds):
        sorted_pred = sorted(pred, key=lambda x: x['score'], reverse=True)
        top_label = sorted_pred[0]['label']
        top_score = sorted_pred[0]['score']

        # FinBERT label ê²°ì •
        if top_label == "neutral" and top_score < threshold:
            finbert_label = sorted_pred[1]['label']
        else:
            finbert_label = top_label

        # Electra label ë§¤í•‘ (0: ë¶€ì •, 1: ì¤‘ë¦½, 2: ê¸ì •)
        electra_label = electra_labels[i]
        electra_label_str = {0: "negative", 1: "neutral", 2: "positive"}[electra_label]

        # ê²°í•© ë¡œì§
        if ("positive" in [finbert_label, electra_label_str]) and not (
                "negative" in [finbert_label, electra_label_str]):
            final_results.append(1)
        elif ("negative" in [finbert_label, electra_label_str]) and not (
                "positive" in [finbert_label, electra_label_str]):
            final_results.append(-1)
        else:
            final_results.append(0)

    return final_results


def extract_top_keywords(texts, top_n=4):
    all_nouns = []
    for text in texts:
        nouns = okt.nouns(text)
        all_nouns.extend(nouns)
    counter = Counter(all_nouns)
    return [word for word, _ in counter.most_common(top_n)]


def show_sample_titles(df):
    print("\n=== ê°ì„±ë³„ ì œëª© ì˜ˆì‹œ ===")
    for sentiment, label in zip(["ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"], [1, 0, -1]):
        samples = df[df['ê°ì„±'] == label]['ì œëª©'].sample(n=min(5, len(df[df['ê°ì„±'] == label])), random_state=42).tolist()
        if samples:
            print(f"\n[{sentiment} ì˜ˆì œ]")
            for title in samples:
                print(f"- {title}")
        else:
            print(f"\n[{sentiment} ì˜ˆì œ] í•´ë‹¹ ê°ì„±ì˜ ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤.")


# ë©”ì¸ ì‹¤í–‰
def main():
    item_code = input("ìˆ˜ì§‘í•  ì¢…ëª© ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    stock_name = input("ì¢…ëª©ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì‚¼ì„±ì „ì): ").strip()

    print(f"{item_code} ({stock_name})ì˜ ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...")
    df = get_all_pages(item_code)
    if df.empty:
        print(f"{item_code}ì˜ ì˜¤ëŠ˜ ë‚ ì§œ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ë°ì´í„° ì „ì²˜ë¦¬ ë° ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...")
    df['ì „ì²˜ë¦¬ëœ ì œëª©'] = df['ì œëª©'].apply(preprocess_text)
    df['ê°ì„±'] = analyze_sentiments_with_dual_model(df['ì „ì²˜ë¦¬ëœ ì œëª©'].tolist(), threshold=0.6)

    sentiment_distribution = df['ê°ì„±'].value_counts(normalize=True)

    result = {
        "stock_code": item_code,
        "stock_name": stock_name,
        "updated_at": datetime.utcnow().isoformat() + "Z",
        "sentiment": {
            "positive": round(sentiment_distribution.get(1, 0.0), 2),
            "neutral": round(sentiment_distribution.get(0, 0.0), 2),
            "negative": round(sentiment_distribution.get(-1, 0.0), 2)
        },
        "top_keywords": extract_top_keywords(df['ì „ì²˜ë¦¬ëœ ì œëª©'])
    }

    print("\n=== JSON ê²°ê³¼ ===")
    print(json.dumps(result, ensure_ascii=False, indent=2))

    show_sample_titles(df)

    # ğŸ”½ CSV ì €ì¥ ì¶”ê°€
    output_filename = f"sentiment_result_{item_code}_{datetime.today().strftime('%Y%m%d')}.csv"
    df.to_csv(output_filename, index=False, encoding='utf-8-sig')
    print(f"\nCSV íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ: {output_filename}")

if __name__ == "__main__":
    main()
